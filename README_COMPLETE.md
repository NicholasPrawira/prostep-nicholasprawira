# Tigaraksa Image Search - Full Stack Application

AI-powered image search application that allows users to search for images based on text descriptions using semantic search technology.

## ğŸ“‹ Table of Contents
- [Project Overview](#project-overview)
- [ğŸ“ Project Structure](#-project-structure)
- [ğŸ› ï¸ Tech Stack](#ï¸-tech-stack)
- [ğŸš€ Quick Start](#-quick-start)
  - [Prerequisites](#prerequisites)
  - [Backend Setup](#backend-setup)
  - [Frontend Setup](#frontend-setup)
- [ğŸ”Œ API Endpoints](#-api-endpoints)
- [ğŸ“ Environment Configuration](#-environment-configuration)
- [ğŸ“Š Features](#-features)
- [ğŸ¯ How It Works](#-how-it-works)
- [ğŸ”„ CLIPScore Evaluation Pipeline](#-clipscore-evaluation-pipeline)
- [ğŸ‘¥ Team](#-team)

## Project Overview

This application enables users to search for images using natural language descriptions. It leverages AI embeddings and vector similarity search to find the most relevant images based on semantic meaning rather than keyword matching.

The system consists of:
- A FastAPI backend that handles image search requests
- A Next.js frontend with a clean, responsive UI
- PostgreSQL with pgvector extension for storing image embeddings
- CLIPScore evaluation pipeline for measuring image-text similarity

## ğŸ“ Project Structure

```
Skripsi/Code - Bab IV/
â”œâ”€â”€ server.py                 # FastAPI backend server
â”œâ”€â”€ evaluation.py             # CLIPScore evaluation pipeline
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ .env                      # Backend environment variables
â”œâ”€â”€ .env.template             # Template for backend environment variables
â”œâ”€â”€ README.md                 # CLIPScore evaluation documentation
â”œâ”€â”€ README_FULLSTACK.md       # Full-stack project documentation
â”œâ”€â”€ README_COMPLETE.md        # This file
â””â”€â”€ frontend/                 # Next.js frontend application
    â”œâ”€â”€ src/
    â”‚   â”œâ”€â”€ app/
    â”‚   â”‚   â”œâ”€â”€ page.tsx      # Main page component
    â”‚   â”‚   â”œâ”€â”€ layout.tsx    # Root layout component
    â”‚   â”‚   â””â”€â”€ globals.css   # Global styles
    â”‚   â””â”€â”€ components/
    â”‚       â”œâ”€â”€ SearchForm.tsx # Search form and results display
    â”‚       â””â”€â”€ ResultsGrid.tsx # Results grid component
    â”œâ”€â”€ package.json          # Frontend dependencies
    â”œâ”€â”€ next.config.js        # Next.js configuration
    â”œâ”€â”€ tailwind.config.ts    # Tailwind CSS configuration
    â”œâ”€â”€ postcss.config.js     # PostCSS configuration
    â”œâ”€â”€ .env.local            # Frontend environment variables
    â””â”€â”€ tsconfig.json         # TypeScript configuration
```

## ğŸ› ï¸ Tech Stack

### Backend
- **FastAPI** - High-performance Python web framework
- **Sentence Transformers** - AI embeddings for semantic search
- **PostgreSQL + pgvector** - Database with vector similarity search
- **Psycopg2** - PostgreSQL adapter for Python
- **Uvicorn** - ASGI server for FastAPI

### Frontend
- **Next.js 14** - React framework with App Router
- **React 18** - JavaScript library for building user interfaces
- **TypeScript** - Typed superset of JavaScript
- **Tailwind CSS** - Utility-first CSS framework
- **Axios** - Promise-based HTTP client

### CLIPScore Evaluation
- **Transformers (Hugging Face)** - CLIP model for image-text similarity
- **PyTorch** - Machine learning framework
- **Supabase** - Cloud database and storage
- **Pillow** - Python Imaging Library

## ğŸš€ Quick Start

### Prerequisites
- Python 3.8+
- Node.js 16+
- npm 8+
- PostgreSQL with pgvector extension
- Supabase account (for database and storage)

### Backend Setup

1. Create a virtual environment and activate it:
   ```bash
   python -m venv .venv
   # Windows
   .\.venv\Scripts\activate
   # macOS/Linux
   source .venv/bin/activate
   ```

2. Install Python dependencies:
   ```bash
   pip install -r requirements.txt
   ```

3. Configure environment variables:
   - Copy `.env.template` to `.env`
   - Fill in your Supabase database URL

4. Run the backend server:
   ```bash
   uvicorn server:app --reload
   ```
   The API will be available at `http://127.0.0.1:8000`

### Frontend Setup

1. Navigate to the frontend directory:
   ```bash
   cd frontend
   ```

2. Install frontend dependencies:
   ```bash
   npm install
   ```

3. Configure frontend environment:
   - Create `.env.local` file
   - Set `NEXT_PUBLIC_API_URL=http://127.0.0.1:8000`

4. Run the development server:
   ```bash
   npm run dev
   ```
   The UI will be available at `http://localhost:3000`

## ğŸ”Œ API Endpoints

- `GET /health` - Health check endpoint
- `GET /search?q=<query>` - Search for images by text description

Example search request:
```bash
curl "http://127.0.0.1:8000/search?q=beautiful+sunset+over+mountains"
```

Response format:
```json
{
  "query": "beautiful sunset over mountains",
  "results": [
    {
      "prompt": "A breathtaking sunset painting with vibrant orange and pink colors",
      "image_url": "https://your-supabase-url.com/storage/v1/object/public/images/image1.jpg",
      "clipscore": 0.324,
      "similarity": 0.872
    }
  ]
}
```

## ğŸ“ Environment Configuration

### Backend (.env)
```env
SUPABASE_DB_URL=postgresql://username:password@host:port/database
```

### Frontend (.env.local)
```env
NEXT_PUBLIC_API_URL=http://127.0.0.1:8000
```

## ğŸ“Š Features

âœ… **AI-powered semantic search** - Find images based on meaning, not just keywords  
âœ… **Image search by text description** - Describe what you're looking for in natural language  
âœ… **Similarity scoring** - See how closely images match your query  
âœ… **CLIPScore evaluation** - Measure image-text alignment quality  
âœ… **CORS enabled** - Secure cross-origin requests  
âœ… **Real-time API responses** - Fast search results  
âœ… **Responsive UI design** - Works on desktop and mobile devices  
âœ… **Error handling & validation** - Graceful error management  

## ğŸ¯ How It Works

1. User enters an image description in the UI
2. Frontend sends the query to the backend API
3. Backend generates embeddings using Sentence Transformers
4. Vector similarity search is performed in PostgreSQL using pgvector
5. Top 5 most relevant results are returned
6. Frontend displays images along with similarity scores

## ğŸ”„ CLIPScore Evaluation Pipeline

The `evaluation.py` script computes CLIP-based similarity (CLIPScore) between stored images and their prompts, then writes scores to a Supabase table.

### How it works:
- Fetches image rows from the `images` table
- Creates signed URLs from Supabase Storage
- Downloads images and computes CLIP similarity against the prompt
- Inserts results into the `clip_scores` table

### Required environment variables:
- `SUPABASE_URL` - Your Supabase project URL
- `SUPABASE_SERVICE_KEY` - Service role key (keep secret)
- `SUPABASE_BUCKET` - Storage bucket name (default: `images`)

### Running the evaluator:
```bash
python evaluation.py
```

### Notes:
- Uses the `openai/clip-vit-base-patch32` model from Hugging Face
- Automatically uses GPU if available (CUDA-enabled PyTorch)
- First run may take time to download the model

## ğŸ‘¥ Team

Developed as part of a thesis project at Universitas Multimedia Nusantara.

---
**Status:** âœ… Ready for development & deployment